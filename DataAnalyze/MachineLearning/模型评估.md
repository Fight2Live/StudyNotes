# 模型评估		

​		算法模型是多种多样的。很多时候对一个需求进行预测或分类时，我们有很多种选择，那么要如何去衡量某个算法对当前场景的效果最好呢，往往都会有一些指标来提供衡量标准。除去指标外还有算法本身的特性也会有所影响

> ​		内容在很大程度都参考 [机器学习评价指标大汇总](https://zhaokv.com/machine_learning/2016/03/ml-metric.html)
>
> ​		与书本：《Hands-On Machine Learning with Scikit-Learn,Keras,and TensorFlow,2nd Edition》作者：Aurelien Geron

## 一、分类



### 1、常用术语

​	1）True positives(真正，TP): 被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数（样本数）；

​	2）False positives(假正，FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数 → **误报**；

​	3）False negatives(假负，FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数 → **漏报**；

​	4）True negatives(真负，TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。

|              |          | **预测类别** |        | **总计** |
| ------------ | -------- | ------------ | ------ | -------- |
| **实际类别** |          | **Yes**      | **No** |          |
|              | **Yes**  | TP           | FN     | P        |
|              | **No**   | FP           | TN     | N        |
|              | **总计** | P'           | N'     | P+N      |



### **2、常规评价指标**

### 	**（1）准确率（Accuracy）**

​			**正确率是很常见的评价指标，即在所有待分类的样本中，被正确分类的样本比例**
$$
Accuracy = \frac{TP + TN}{P + N}
$$

虽然准确率能够判断总的正确率，但是在**样本不均衡**的情况下，并不能作为很好的指标来衡量结果。

比如在样本集中，正样本有90个，负样本有10个，样本是严重的不均衡。对于这种情况，我们只需要将全部样本预测为正样本，就能得到90%的准确率，但是完全没有意义。对于新数据，完全体现不出准确率。因此，在样本不平衡的情况下，得到的高准确率没有任何意义，此时准确率就会失效。所以，我们需要寻找新的指标来评价模型的优劣。

### 	**（2）精确率（precision）**

​			**在所有被预测为正的样本中，实际也为正的样本概率**
$$
Precision = \frac{TP}{TP+FP}
$$

### 	**（3）召回率（Recall）**

​			**在所有实际为正的样本中，预测结果也为正的样本概率**
$$
Recall = \frac{TP}{TP+FN}
$$

> **假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。**
>
> 
>
> **准确率(accuracy) = 预测对的/所有 = (TP+TN)/(P+N) = 70%**
> **精确率(precision) = TP/(TP+FP) = 80%**
> **召回率(recall) = TP/(TP+FN) = 2/3**



### 	**（4）其他评价指标**

- **鲁棒性：处理缺失值和异常值的能力**
- **可解释性：分类器预测标准的可理解性。像决策树产生的规则就是易理解的，而神经网络则是很多个不易理解的参数。**
- **可扩展性：处理大数据集的能力**
- **计算速度：分类器训练和预测所需要的时间**



​		**对于某个具体的分类器而言，我们不可能同时提高所有上面介绍的指标，当然，如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如地震预测，没有谁能准确预测地震的发生，但我们能容忍一定程度的误报，假设1000次预测中，有5次预测为发现地震，其中一次真的发生了地震，而其他4次为误报，那么正确率从原来的999/1000=99.9%下降到996/1000=99.6，但召回率从0/1=0%上升为1/1=100%，这样虽然谎报了几次地震，但真的地震来临时，我们没有错过，这样的分类器才是我们想要的，在一定正确率的前提下，我们要求分类器的召回率尽可能的高。**



### **3、ROC曲线**

**Y轴：**
$$
真正率（TPR）=灵敏度（Sensitivity）=召回率= \frac{TP}{TP+FN}
$$


**X轴：**
$$
假正率（FPR）=1-特异度（Specificity）= \frac{FP}{FP+TN}
$$
TPR和FPR分别是基于实际表现1、0出发的，也就是说在实际的正样本和负样本中来观察相关概率问题。因此，无论样本是否均衡，都不会被影响



#### 3.1、AUC

​		直接来讲，AUC即ROC曲线下的面积。但严谨的定义为：随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值， 比 分类器输出该负样本为正的那个概率值， 要大的可能性。

​		[参考1](https://www.zhihu.com/question/39840928)



### **4、对数损失**



### **5、铰链损失**



### **6、混淆矩阵**

​		可以清晰的反映出真实值与预测值相互吻合的部分，也可以反映出不对的部分。例子见上面的常用术语



### **7、Kappa系数**



### **8、海明距离**



### **9、杰卡3德相似系数**



### **10、多标签排序**



### 11、F1分数

又称平衡F分数，被定义为精确率与召回率的调和平均数
$$
F_1=2·\frac{precision·recall}{precision+recall}
$$
更一般的，我们将1换成β，有：
$$
F_\beta=(1+\beta)·\frac{precision·recall}{\beta^2·precision+recall}
$$


所以类似的还有F2分数和F0.5分数。

F2分数中，召回率的权重高于精确率，

而F0.5分数中，精确率的权重高于召回率







## **二、回归**



## **三、聚类**