# KNN

​		KNN是现实中经常采用的一种基于距离的分类算法，同时它的思想也是比较简单的。



## 原理		

​		有一个样本训练集，并且样本集中中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中前K个特征最相似数据的分类标签，选择这K个标签中出现次数最多的标签，作为新数据的分类结果

​	**优点：**精度高、对异常值不敏感

​	**缺点**计算复杂度高，空间复杂度高

​	**适用数据范围：**数值型，标称型



## 常用的距离

### 欧氏距离

​		**欧氏距离**，指的是在m维空间中两个点之间的真是距离，或者向量的自然长度（即该点到原点的距离）。在二维和三位空间中，欧氏距离就是两点间的实际距离（可简单的理解为位移量）

​	

​		n维空间中：
$$
d(x,y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\cdots+(x_n-y_n)^2} = \sqrt{\sum^n_{i=1}(x_i-y_i)^2}
$$
​		

### 曼哈顿距离







# 如何选定一个很好的k值？



## 1、网格搜索

​		GridSearch和CV，即网格搜索和交叉验证

​		网格搜索算法是一种通过遍历给定的参数组合来优化模型表现的方法

**为何使用**：超参数选择不恰当，就会出现欠拟合或者过拟合的问题

**内容：** 网格搜索，搜索的是**参数**，即在指定的参数范围内，按步长依次调整参数，利用调整的参数**训练学习器**，从所有的参数中找到在验证集上**精度最高**的参数，这其实是一个训练和比较的过程。



Grid Search：一种调参手段；穷举搜索：在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，表现最好的参数就是最终的结果

**用法**：网格搜索适用于三四个（或者更少）的超参数（当超参数的数量增长时，网格搜索的计算复杂度会呈现指数增长，这时候则使用**随机搜索）**，用户列出一个较小的超参数值域，这些超参数至于的笛卡尔积（排列组合）为一组组超参数。网格搜索算法使用每组超参数训练模型并挑选验证集误差最小的超参数组合

**缺点：**遍历所有组合，比较耗时



## 2、交叉验证
