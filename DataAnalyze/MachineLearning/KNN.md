# KNN

​		KNN是现实中经常采用的一种基于距离的分类算法，同时它的思想也是比较简单的。



## 原理		

​		有一个样本训练集，并且样本集中中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中前K个特征最相似数据的分类标签，选择这K个标签中出现次数最多的标签，作为新数据的分类结果

​	**优点：**精度高、对异常值不敏感

​	**缺点**计算复杂度高，空间复杂度高

​	**适用数据范围：**数值型，标称型



## 常用的距离

### 欧氏距离

​		**欧氏距离**，指的是在m维空间中两个点之间的真是距离，或者向量的自然长度（即该点到原点的距离）。在二维和三位空间中，欧氏距离就是两点间的实际距离（可简单的理解为位移量）

​	

​		n维空间中：
$$
d(x,y)=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\cdots+(x_n-y_n)^2} = \sqrt{\sum^n_{i=1}(x_i-y_i)^2}
$$
​		

### 曼哈顿距离



## 数据处理

​		要注意的是，当数据维度很多，且其中存在差值很大的数据时，该类数据会对结果产生决定性的影响。所以为了避免这种情况发生，我们通常需要对数据进行**归一化**处理。常用的归一化方式有：

### 1、min-max标准化

​		也称为离差标准化，是对原始数据的线性变换，**使结果值映射到[0 - 1]之间**。
$$
newValue = \frac{oldValue - min}{max-min}
$$


### 2、Z-score标准化

​		这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：
$$
newValue = \frac{oldValue-μ}{σ}
$$
​		其中μ是均值，σ是标准差



