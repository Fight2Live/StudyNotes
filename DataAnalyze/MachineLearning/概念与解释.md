# 概念与解释



## 算法分类

### 监督学习：

在监督学习的过程中，我们只需要给定输入样本集，机器就可以从中推演出指定目标变量的可能结果。

**根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。**

需要有样本集，通过分析样本集中的 **特征** 和 **标签** 去得到一个演算模型，可以由已知的特征去得到未知的标签。

#### 分类算法：

1. K-NN，决策树，朴素贝叶斯，逻辑回归，支持向量机，Adaboost





### 无监督学习：

与监督学习相反，它不需要提前给定样本集去训练模型。它可以把标签未知的数据，根据一定规则 **聚拢** 起来，形成一簇又一簇。

**即，我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。**

#### 聚类算法：

1. K-Mean，Apriori，FP-growth，





## 如何选择合适的算法？

### 1、首先考虑使用机器学习算法的**目的** 。

如果想要预测目标变量的值，则可以选择监督学习算法；否则选择无监督学习



### 2、确定目标变量类型

如果目标变量是离散型，如是/否，A/B/C，红/绿/蓝等，可以选择**分类器算法**；

如果目标变量是连续型数值，如0.0~100.00，-999~999等，则需要选择**回归算法**。

如果不想预测目标变量的值，则可以选择**无监督学习算法** 。

1. 进一步分析是否需要将数据划分为离散的组。如果这是唯一需求，则使用聚类算法；
2. 如果还需要估计数据与每个分组的相似程度，则需要使用密度估计算法。
3. 数据分析



### 3、了解数据特性

主要了解数据的以下特性：

1. 特征值是离散型变量还是连续型变量？
2. 特征值中是否存在缺失值？何种原因造成的缺失？
3. 数据中是否存在异常值？
4. 某个特征发生的频率如何？
5. 等等……

通过分析数据，我们可以缩短选择机器学习算法的时间，选择到更加合适的，性能更好的，并让思路更加清晰



## 衡量指标

​		算法模型是多种多样的。很多时候对一个需求进行预测或分类时，我们有很多种选择，那么要如何去衡量某个算法对当前场景的效果最好呢，往往都会有一些指标来提供衡量标准。除去指标外还有算法本身的特性也会有所影响

> ​		内容在很大程度都参考 [机器学习评价指标大汇总](https://zhaokv.com/machine_learning/2016/03/ml-metric.html)
>
> ​		与书本：《Hands-On Machine Learning with Scikit-Learn,Keras,and TensorFlow,2nd Edition》作者：Aurelien Geron

### 一、分类



#### 1、常用术语

​	1）True positives(真正，TP): 被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数（样本数）；

​	2）False positives(假正，FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数 → **误报**；

​	3）False negatives(假负，FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数 → **漏报**；

​	4）True negatives(真负，TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。

|              |          | **预测类别** |        | **总计** |
| ------------ | -------- | ------------ | ------ | -------- |
| **实际类别** |          | **Yes**      | **No** |          |
|              | **Yes**  | TP           | FN     | P        |
|              | **No**   | FP           | TN     | N        |
|              | **总计** | P'           | N'     | P+N      |



#### **2、常规评价指标**

##### 	**（1）准确率（Accuracy）**

​			**正确率是很常见的评价指标，即在所有待分类的样本中，被正确分类的样本比例**
$$
Accuracy = \frac{TP + TN}{P + N}
$$

虽然准确率能够判断总的正确率，但是在**样本不均衡**的情况下，并不能作为很好的指标来衡量结果。

比如在样本集中，正样本有90个，负样本有10个，样本是严重的不均衡。对于这种情况，我们只需要将全部样本预测为正样本，就能得到90%的准确率，但是完全没有意义。对于新数据，完全体现不出准确率。因此，在样本不平衡的情况下，得到的高准确率没有任何意义，此时准确率就会失效。所以，我们需要寻找新的指标来评价模型的优劣。

##### 	**（2）精确率（precision）**

​			**在所有被预测为正的样本中，实际也为正的样本概率**
$$
Precision = \frac{TP}{TP+FP}
$$

##### 	**（3）召回率（Recall）**

​			**在所有实际为正的样本中，预测结果也为正的样本概率**
$$
Recall = \frac{TP}{TP+FN}
$$

> **假设我们手上有60个正样本，40个负样本，我们要找出所有的正样本，系统查找出50个，其中只有40个是真正的正样本，计算上述各指标。**
>
> 
>
> **准确率(accuracy) = 预测对的/所有 = (TP+TN)/(P+N) = 70%**
> **精确率(precision) = TP/(TP+FP) = 80%**
> **召回率(recall) = TP/(TP+FN) = 2/3**



##### 	**（4）其他评价指标**

- **鲁棒性：处理缺失值和异常值的能力**
- **可解释性：分类器预测标准的可理解性。像决策树产生的规则就是易理解的，而神经网络则是很多个不易理解的参数。**
- **可扩展性：处理大数据集的能力**
- **计算速度：分类器训练和预测所需要的时间**



​		**对于某个具体的分类器而言，我们不可能同时提高所有上面介绍的指标，当然，如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如地震预测，没有谁能准确预测地震的发生，但我们能容忍一定程度的误报，假设1000次预测中，有5次预测为发现地震，其中一次真的发生了地震，而其他4次为误报，那么正确率从原来的999/1000=99.9%下降到996/1000=99.6，但召回率从0/1=0%上升为1/1=100%，这样虽然谎报了几次地震，但真的地震来临时，我们没有错过，这样的分类器才是我们想要的，在一定正确率的前提下，我们要求分类器的召回率尽可能的高。**



#### **3、ROC曲线**

真正率（TPR）=灵敏度（Sensitivity）=召回率=TP/(TP+FN)

假正率（FPR）=1-特异度（Specificity）=FP/(FP+TN)

TPR和FPR分别是基于实际表现1、0出发的，也就是说在实际的正样本和负样本中来观察相关概率问题。因此，无论样本是否均衡，都不会被影响







#### **4、对数损失**



#### **5、铰链损失**



#### **6、混淆矩阵**

可以清晰的反映出真实值与预测值相互吻合的部分，也可以反映出不对的部分。



#### **7、Kappa系数**



#### **8、海明距离**



#### **9、杰卡德相似系数**



#### **10、多标签排序**



#### 11、F1分数

又称平衡F分数，被定义为精确率与召回率的调和平均数
$$
F_1=2·\frac{precision·recall}{precision+recall}
$$
更一般的，我们将1换成β，有：
$$
F_\beta=(1+\beta)·\frac{precision·recall}{\beta^2·precision+recall}
$$


所以类似的还有F2分数和F0.5分数。

F2分数中，召回率的权重高于精确率，

而F0.5分数中，精确率的权重高于召回率







### **二、回归**



### **三、聚类**