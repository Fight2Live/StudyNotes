# 决策树

​		决策树是从数据中生成分类器的一个重要的、基本的和有效的方法。决策树分类方法才有自顶向下的递归方式，它把一组无次序的事例整理成树形结构，并由树结构导出分类规则。

​		在决策树的内部结点进行属性值的比较并根据不同的属性值判断从该结点向下的分支，在叶结点得到结论。



## 原理

​		决策树是应用最广泛的逻辑方法之一，它是从一组无次序、无规则的事例中推理出决策树表示形式的分类规则。

​		它是通过一系列规则对数据进行分类的过程。它提供一种在什么条件下会得到什么值的类似规则的方法。

​		决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。



**优点：** 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征的数据。

**缺点：** 可能会产生过度匹配的问题。

**适用数据类型：** 数值型和标称型。



## 基于信息论的三种算法

### 	ID3

​		ID3算法最早是由罗斯昆（J. Ross Quinlan）于1975年在悉尼大学提出的一种分类预测算法，算法的核心是**“信息熵”**。ID3算法通过计算每个属性的信息增益，**认为信息增益高的是好属性**，每次划分选取信息增益**最高**的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。



### 	C4.5

​		C4.5是ID3的一个改进算法，继承了ID3算法的优点。C4.5算法用**”信息增益率“**来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不完整数据进行处理。C4.5算法产生的分类规则易于理解、准确率较高；但效率低，因树构造过程中，需要对数据集进行多次的顺序扫描和排序。也是因为必须多次数据集扫描，C4.5只适合于能够驻留于内存的数据集。

​	

### CART

​		CART（分类和回归树，Classification and Regression Tree）算法使用的是**基尼系数**来进行决策分类。基尼系数越小，不纯度越低，特征越好。

​		CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。



## 连续值的处理

​		实际应用中是会经常遇到连续型特征的，这时就需要将其进行离散化处理

​		**有一点需要注意的是：与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。**

### 信息增益二分法

​		给定训练集D和连续属性a，假定a在D上出现了n个不同的取值，先把这些值从小到大进行排序，记为{a1, a2, ..., an}。基于划分点t可将D分为子集D_t^-和D_t^+，其中D_T^-是包含那些在属性a上取值不大于t的样本，D_t^+是值在属性a上大于t的样本。显然对于相邻的属性值ai与ai+1来说，t在区间[ai, ai+1)中取任意值所产生的划分结果相同。因此对于连续属性a，我们可考察包含n-1个元素的候选划分点的集合
$$
T_a = \{\frac{a^i+a^{i+1}}{2}|1≤i≤n-1\}
$$
即把区间[ai, ai+1)的中卫点作为候选划分点。然后，我们就可以像前面处理离散属性值那样来考虑这些划分点，选择最优的划分点进行样本集合划分：
$$
Gain(D,a) = maxGain(D, a, t)_{t\in{T_a}}=max_{t\in{T_a}}\left( E(D)-\sum_{\lambda\in{\{-,+\}}}E(D_t^\lambda))\right)
$$
其中Gain（D，a，t）是样本集D基于划分点t二分后的信息增益。划分的时侯选择最大的G作为划分点。

这个权重的概念，在上面ID3算法的信息熵计算中，其实已经包含了，只不过它的权重是在总样本中算，而这里是在无缺失样本中算。



## 决策树生成步骤（ID3，C4.5）

1. 以代表训练样本的单个节点开始建树。

2. 如果样本都在同一类，则该节点称为树叶，并用该类标记。

3. 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性。该属性成为该节点的&quot;测试&quot;或&quot;判定&quot;属性。值得注意的是，**这类算法中，所有的属性都是分类的，即取离散值的。连续值的属性必须离散化。**

4. 对测试属性的每个已知的值，创建一个分支，并据此划分样本。

5. 算法使用同样的过程，递归地形成每个划分上的样本决策树。一旦一个属性在一个节点上，就不必考虑该节点的任何后代。

6. 递归划分步骤，当下列条件之一成立时停止：

   - 给定节点的所有样本属于同一类。

   - 没有剩余属性可以用来进一步划分样本。在此情况下，采用多数表决。这涉及将给定的结点转成树叶，并用samples中的多数所在的类额标记它。换一种方式，可以存放结点样本的类分布。

   - 分支test\_attribute = Ai没有样本。这种情况下以samples中多数所在的类别创建一个树叶。



## ID3算法

​		决策树中每一个非叶节点，对应着一个非类别属性，树枝代表这个属性的值。一个叶结点代表从树根到叶结点之间的路径对应的记录所属的类别属性值。

每一个非叶节点都将与属性中句又最大信息量的非类别属性相关联。

采用信息增益来选择出能够最好的将样本分类的属性。

​		**一个属性的熵越大，它蕴含的不确定信息越大，越不利于数据的分类**。因此，ID3总是选择具有**最高信息增益**（最小熵）的属性作为当前结点的测试属性。



**期望信息：**

​		设D是d个数据样本的集合。假定类标号属性具有m个不同的值，定义m个不同类Ci（i = 1，2，……，m）。设si是类Ci中的样本数。对一个给定的样本分类所需的期望信息I为：
$$
I = - \sum^n_{n=1}P_nlog_2P_n
$$

**信息熵：**

​	

​		设属性A具有v个不同值{a1，a2，a3，……，av}，可以用属性A将D换分为j个子集{D1,D2,D3,……，Dj}，其中Dj为数据集D在字段A上值为aj的集合。

​		设dij是子集Dj中类Yi的样本数，则由字段A划分成子集的信息熵为：
$$
E（A）= - \sum^v_{j=1}\frac{d_{1j} + d_{2j} +......+ d_{ij}}{d}·I(d_{1j} , d_{2j}, ......, d_{ij})
$$
其中(d1j+d2j+...+dij)\d是指在总体样本中，特征A=aj下的占比。缺失值数量不会算入分子中，该系数相当于该特征的权重。

**信息增益：**

​	抽象来说，熵值越大时，对应的样本数据越混乱，类别越多，纯度越低；反之则越有序，类别越少，纯度越高。

​	所以由期望信息和熵值可以得到对应的信息增益值，信息增益越大，则表示使用的特征a对数据集划分所获得的“**纯度提升**”越大。所以信息增益可以用于决策树划分属性的选择，其实就是选择信息增益最大的属性，对于在A上分支将获得的信息增益G为：
$$
Gain(A) = I(d_{1} , d_{2}, ......, d_{i}) - E(A)
$$

### 	缺点：

​			对可取数值多的属性有偏好，比如我们取index作为一个特征去分类，那么将产生一颗有index个节点的没有泛化能力的树，






## C4.5算法

在ID3的基础上，对计算的信息增益多除以当前特征的期望信息，来表示该特征的信息增益率。能规避对多属性特征的偏置。
$$
GainRatio(A)=\frac{Gain(A)}{SplitI(A)}
$$

$$
SplitI(A)=-\sum^v_{j=1}p_jlog_2(p_j)
$$









## CART

### 分类

基于基尼指数。数值越大，样本集合的不确定性也就越大，这一点与熵的概念比较类似。所以我们取基尼指数最小的特征作为划分点

假设有m个类，样本属于第j类的概率为pk，则概率分布的基尼指数为：
$$
Gini(p)=\sum^m_{j=1}p_j(1-p_j)=1-\sum^m_{j=1}p^2_j
$$
如果是二分类问题，则计算公式为：
$$
Gini(p) = 2·p·(1-p)
$$
原始数据集D因为共有m个类，所以数据集D的基尼系数为：
$$
Gini(D)=\sum^{m}_{j=1}\frac{D_j}{D}Gini(D_j)
$$

现有数据集D，特征集为X={X1, X2, ..., Xn}，现取X1对D进行分类，X1有v个取值{a1, a2, ..., av}，先根据a1将样本集分为D1(X1=a1)，D2(X1!=a1)，那么在特征X1的条件下，样本D的基尼系数为：
$$
Gini(D,X_1) = \frac{|D1|}{|D|}Gini(D1) + \frac{|D2|}{|D|}Gini(D2) 
$$


**注意：**在CART中，无论是连续值还是离散值，被用于划分的特征，只要还有值没有划完，皆可继续参与后续的样本划分。

**注意：**算法的停止条件：

1. 是当前样本数小于预定阈值
2. 或者样本集的基尼指数小于预定阈值
3. 或者没有更多特征





### 回归

这时采用最小二乘法，寻找使损失函数最小的那个点作为划分依据，常用的损失函数是MSR（均方误差）

计算规则与分类树类似，取特征A进行计算，对值进行排序，从第一个值开始取，将当前样本集划分为两个子集，子集中预测的y值y_hat为集合中所有y值的均值。











## 剪枝



### 预剪枝

​		在决策树生成过程中进行剪枝操作，比如在选择特征进行划分时，对划分出来的子节点使用多数表决来作为暂时的叶子节点，之后使用测试集在这颗子树上进行分类，若准确率没有提高，则剪掉这个子树







### 后剪枝

​		首先生成与训练数据完全拟合的一颗决策树，然后从树的叶子开始剪枝，逐步向根的方向剪。剪枝时要用一个测试数据集合，如果存在某个叶子剪去后使得在测试集上的准确度或其他测度不降低，则剪去。





