# 决策树

​		决策树是从数据中生成分类器的一个重要的、基本的和有效的方法。决策树分类方法才有自顶向下的递归方式，它把一组无次序的事例整理成树形结构，并由树结构导出分类规则。

​		在决策树的内部结点进行属性值的比较并根据不同的属性值判断从该结点向下的分支，在叶结点得到结论。



## 原理

​		决策树是应用最广泛的逻辑方法之一，它是从一组无次序、无规则的事例中推理出决策树表示形式的分类规则。

​		它是通过一系列规则对数据进行分类的过程。它提供一种在什么条件下会得到什么值的类似规则的方法。

​		决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。



**优点：** 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征的数据。

**缺点：** 可能会产生过度匹配的问题。

**适用数据类型：** 数值型和标称型。



## 基于信息论的三种算法

### 	ID3

​		ID3算法最早是由罗斯昆（J. Ross Quinlan）于1975年在悉尼大学提出的一种分类预测算法，算法的核心是**“信息熵”**。ID3算法通过计算每个属性的信息增益，**认为信息增益高的是好属性**，每次划分选取信息增益**最高**的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。



### 	C4.5

​		C4.5是ID3的一个改进算法，继承了ID3算法的优点。C4.5算法用**”信息增益率“**来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不完整数据进行处理。C4.5算法产生的分类规则易于理解、准确率较高；但效率低，因树构造过程中，需要对数据集进行多次的顺序扫描和排序。也是因为必须多次数据集扫描，C4.5只适合于能够驻留于内存的数据集。

​	

### CART

​		CART（分类和回归树，Classification and Regression Tree）算法使用的是**基尼系数**来进行决策分类。基尼系数越小，不纯度越低，特征越好。

​		CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。



## 决策树生成步骤（ID3，C4.5）

1. 以代表训练样本的单个节点开始建树。

2. 如果样本都在同一类，则该节点称为树叶，并用该类标记。

3. 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性。该属性成为该节点的&quot;测试&quot;或&quot;判定&quot;属性。值得注意的是，**这类算法中，所有的属性都是分类的，即取离散值的。连续值的属性必须离散化。**

4. 对测试属性的每个已知的值，创建一个分支，并据此划分样本。

5. 算法使用同样的过程，递归地形成每个划分上的样本决策树。一旦一个属性在一个节点上，就不必考虑该节点的任何后代。

6. 递归划分步骤，当下列条件之一成立时停止：

   - 给定节点的所有样本属于同一类。

   - 没有剩余属性可以用来进一步划分样本。在此情况下，采用多数表决。这涉及将给定的结点转成树叶，并用samples中的多数所在的类额标记它。换一种方式，可以存放结点样本的类分布。

   - 分支test\_attribute = Ai没有样本。这种情况下以samples中多数所在的类别创建一个树叶。



## ID3算法

​		决策树中每一个非叶节点，对应着一个非类别属性，树枝代表这个属性的值。一个叶结点代表从树根到叶结点之间的路径对应的记录所属的类别属性值。

每一个非叶节点都将与属性中句又最大信息量的非类别属性相关联。

采用信息增益来选择出能够最好的将样本分类的属性。

​		**一个属性的熵越大，它蕴含的不确定信息越大，越有利于数据的分类**。因此，ID3总是选择具有**最高信息增益**（最大熵）的属性作为当前结点的测试属性。



**期望信息：**

​		设D是d个数据样本的集合。假定类标号属性具有m个不同的值，定义m个不同类Ci（i = 1，2，……，m）。设si是类Ci中的样本数。对一个给定的样本分类所需的期望信息I为：
$$
I = - \sum^n_{n=1}P_nlog_2P_n
$$


**信息熵：**

​	

​		设属性A具有v个不同值{a1，a2，a3，……，av}，可以用属性A将D换分为j个子集{D1,D2,D3,……，Dj}，其中Dj为数据集D在字段A上值为aj的集合。

​		设dij是子集Dj中类Yi的样本数，则由字段A划分成子集的信息熵为：
$$
E（A）= - \sum^v_{j=1}\frac{d_{1j} + d_{2j} +......+ d_{ij}}{d}·I(d_{1j} , d_{2j}, ......, d_{ij})
$$
**信息增益：**

​	抽象来说，熵值越大时，对应的样本数据越混乱，类别越多，纯度越低；反之则越有序，类别越少，纯度越高。

​	所以由期望信息和熵值可以得到对应的信息增益值，对于在A上分支将获得的信息增益G为：
$$
Gain(A) = I(d_{1j} , d_{2j}, ......, d_{ij}) - E(A)
$$








## C4.5算法









## CART















