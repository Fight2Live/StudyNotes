# 机器学习


### 准确率：

往通过 **训练样本集** 训练得到的模型注入 **总（测试**** + ****训练）样本集** ，

准确率 = ![](RackMultipart20210917-4-1o0v95_html_3e3985a7facc55eb.gif)

### 精确率：

往通过 **训练样本集** 训练得到的模型注入 **测试样本集** ，  
精确率 = ![](RackMultipart20210917-4-1o0v95_html_ede2bdb4863a35aa.gif)

### 召回率：

往通过 **训练样本集** 训练得到的模型注入 **训练样本集** ，  
召回率 = ![](RackMultipart20210917-4-1o0v95_html_4cd2b5cbf3cebf42.gif)

## 监督学习：

在监督学习的过程中，我们只需要给定输入样本集，机器就可以从中推演出指定目标变量的可能结果。

**根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。**

需要有样本集，通过分析样本集中的 **特征** 和 **标签** 去得到一个演算模型，可以由已知的特征去得到未知的标签。

### 分类算法：

1. NN，决策树，朴素贝叶斯，逻辑回归，支持向量机，AdaBoost

## 无监督学习：

与监督学习相反，它不需要提前给定样本集去训练模型。它可以把标签未知的数据，根据一定规则 **聚拢** 起来，形成一簇又一簇。

**即，我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。**

### 聚类算法：

1. Mean，Apriori，FP-growth，

## 如何选择合适的算法？

### 首先考虑使用机器学习算法的 **目的** 。

如果想要预测目标变量的值，则可以选择监督学习算法；否则选择无监督学习

### 确定**目标变量类型**

如果目标变量是离散型，如是/否，A/B/C，红/绿/蓝等，可以选择分类器算法；

如果目标变量是连续型数值，如0.0~100.00，-999~999等，则需要选择回归算法。

如果不想预测目标变量的值，则可以选择 **无监督学习算法** 。

1. 进一步分析是否需要将数据划分为离散的组。如果这是唯一需求，则使用聚类算法；
2. 如果还需要估计数据与每个分组的相似程度，则需要使用密度估计算法。

1. 数据分析

主要了解数据的以下特性：

1. 特征值是离散型变量还是连续型变量？
2. 特征值中是否存在缺失值？何种原因造成的缺失？
3. 数据中是否存在异常值？
4. 某个特征发生的频率如何？
5. 等等……

通过分析数据，我们可以缩短选择机器学习算法的时间，选择到更加合适的，性能更好的，并让思路更加清晰


## NN算法

**优点：** 精度高、对异常值不敏感、无数据输入假定。

**缺点：** 计算复杂度高、空间复杂度高。

**适用数据范围：** 数值型和标称型。

**原理：** 有一个样本训练集，并且样本集中中每个数据都存在标签，即我们直到样本集中每一数据与所属分类的对应关系。输入没有标签的数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中前K个特征最相似数据的分类标签，选择这K个标签中出现次数最多的标签，作为新数据的分类结果。

**常用的距离：**

**欧氏距离：**

D为两个 **向量** 点xA，xB之间的距离

D= ![](RackMultipart20210917-4-1o0v95_html_a5e11245d42e022b.gif)

要注意的是，如果有多维数据，且其中存在差值很大的数据时，会对结果产生决定性的影响，所以我们还需要对数据进行 **归一化** 。常用的归一化式子为 **：**

newValue = (oldValue - min)/(max-min)

**曼哈顿距离：**

标明两个点在标准坐标系上的，绝对轴距的总和

![](RackMultipart20210917-4-1o0v95_html_10973b4ef7b59f57.gif)

![](RackMultipart20210917-4-1o0v95_html_26cb2a4ec70c2df2.png)

（红，蓝，黄 为曼哈顿距离； 绿线为欧氏距离）

## **决策树**

**优点：** 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征的数据。

**缺点：** 可能会产生过度匹配的问题。

**适用数据类型：** 数值型和标称型。

### 决策树生成

1. 以代表训练样本的单个节点开始建树。
2. 如果样本都在同一类，则该节点称为树叶，并用该类标记。
3. 否则，算法使用称为信息增益的基于熵的度量作为启发信息，选择能够最好地将样本分类的属性。该属性成为该节点的&quot;测试&quot;或&quot;判定&quot;属性。
4. 对测试属性的每个已知的值，创建一个分支，并据此划分样本。
5. 算法使用同样的过程，递归地形成每个划分上的样本决策树。一旦一个属性在一个节点上，就不必考虑该节点的任何后代。
6. 递归划分步骤，当下列条件之一成立时停止：
  1. 给定节点的所有样本属于同一类。
  2. 没有剩余属性可以用来进一步划分样本。
  3. 分是test\_attribute = ai没有样本。这种情况下以samples中多数所在的类别创建一个树叶。

### ID3算法

决策树中每一个非叶节点，对应着一个非类别属性，树枝代表这个属性的值。一个叶结点代表从树根到叶结点之间的路径对应的记录所属的类别属性值。

每一个非叶节点都将与属性中句又最大信息量的非类别属性相关联。

采用信息增益来选择出能够最好的将样本分类的属性。

一个属性的熵越大，它蕴含的不确定信息越大，越有利于数据的分类。因此，ID3总是选择具有最高信息增益（最大熵）的属性作为当前结点的测试属性。

**信息增益：**

抽象来说，熵值越大时，对应的样本数据越混乱，类别越多，纯度越低；反之则越有序，类别越少，纯度越高。

决策树中，样本集合D中第n类样本所占的比例Pn（n=1,2,...,|Y|），|Y|为样本分类的个数，则D的信息增益为：

![](RackMultipart20210917-4-1o0v95_html_1a5c201484d4c703.gif)

**信息熵：**

设属性A具有v个不同值{a1，a2，a3，……，av}，可以用属性A将D换分为j个子集{D1,D2,D3,……，Dj}，其中Dj为数据集D在字段A上值为aj的集合。

设Dij是子集Dj中类Yi的样本数，则由字段A划分成子集的信息熵为：

![](RackMultipart20210917-4-1o0v95_html_9eb1a9b43a5e84d1.gif)

### C4.5算法

# 随机森林

机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。

# 贝叶斯分类

**名词解释：**

P（H）：H的先验概率。

P（X | H）：假设H成立的情况下，观察到X的概率。

P（H | X）：给定观测样本X，假定H成立的概率。或称条件X下H的后验概率。

![](RackMultipart20210917-4-1o0v95_html_613204ceb6dc2ae.gif)

贝叶斯分类器对 **完全独立** 的数据，和 **函数依赖** 的数据来说有很好的分类效果。

# 数据降维技术

**主成分分析（**** PCA ****）**

**因子分析（**** FA ****）**

**独立成分分析（**** ICA)**

##


## PCA降维

优点：降低数据的复杂性，识别最重要的多个特征。

缺点：不一定需要，且可能损失有用信息

适用数据类型：数值型数据

主成分分析（Principal Component Analysis，PCA）。

在PCA中，数据从原来的坐标系转换到了新的坐标系，而新坐标系的选择是由数据本身决定的。

第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。

该过程一直重复，重复次数为原始数据中特征的数目。