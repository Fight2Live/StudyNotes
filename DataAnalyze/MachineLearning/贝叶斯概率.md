# 贝叶斯概率

​		基于贝叶斯概率理论，我们可以得到一个很基础的朴素贝叶斯模型



## 贝叶斯定理

### 		涉及到的概念解释

#### 			先验概率

​					事情A**还没有**发生，要求这件事情发生的可能性的大小。记做P(A)

#### 			后验概率

​					事情A**已经**发生，要求这件事情发生的原因是由某个因素B引起的可能性的大小，实际上就是条件概率，记做P(A|B)

> 也许可以这样理解，先验概率是在所有样本中，事件A发生的概率。后验概率则是在事件B发生的前提下，事件A发生的概率

​		

​		假设H[1], H[2]…H[n]互斥且构成一个完全事件，已知它们的概率P(H[i])（i=1,2…n），现观察到某事件A与H[1], H[2]…H[n]相伴随机出现，且已知条件概率P(A|H[i])，那么有 
$$
P(H_i|A)=\frac{P(H_i)P(A|H_i)}{\sum^n_{j=1}P(H_j)P(A|H_j)}
$$
​		避免了只使用先验概率的主观偏见，也避免了单独使用样本信息的过拟合现象。贝叶斯分类算法在数据集较大的情况下表现出较高的准确率，同时算法本身也比较简单。



## 朴素贝叶斯

### 	原理

​		每个数据样本用一个n维特征向量X={x1, x2, ......, xm}表示，分别描述对n各属性A1, A2, ......, An样本的n各度量，类别标签为Y={y1, y2, ......, yn}，对于y_i的后验概率有
$$
P(y_i|X)=\frac{P(y_i)P(X|y_i)}{P(X)}
$$
因为我们假设了X中各特征相互独立，所以有
$$
P(X|y_i)=\prod^m_{k=1}P(x_k|y_i)
$$
所以我们可以得到一个样本数据属于类别y_i的朴素贝叶斯公式
$$
P(y_i|X)=\frac{P(y_i)\prod^m_{k=1}P(x_k|y_i)}{\prod^m_{k=1}P(x_k)}
$$






- 它有一个前提假设，就是**假设各个特征是相互独立**的。后面的计算都是基于这个假设来的。所以它在理论上拥有着最小的误差率，但是实际问题中每个特征之间或多或少都有相关性，所以误差就出来了。在实际的应用中还需要进一步优化。



### 种类

如果特征Ak是离散属性，则P(xk|yi)=sik/si，其中sik是在特征Ak上具有值xk的类yi的训练样本数，而si是yi中的训练样本数

#### 高斯分布

对于特征A_k，如果是连续值属性，则通常假定该属性服从高斯分布，即：
$$
P(x_k|y_i)=g(x_k, \mu_k, \sigma_k)=\frac{1}{\sqrt{2\pi\sigma_{y_i}}}e^{\frac{(x_k-\mu_yi)^2}{2\sigma^2_yi}}
$$
其中μ是均值，σ为标准差。



#### 伯努利分布





#### 多项式分布





## 贝叶斯优化





















