# 数据预处理

​		海量的数据样本中，一定会存在一些值需要处理的，比如缺失值，比如异常值、离群点，比如数值大的数据

​		对初始数据进行完整的处理后，得到的数据质量会更高





## 缺失值

> 这里我有个疑问，就是：
>
> 现在有一个样本集A，其中的i列是数值型特征，且有空值，那么在计算该列特征的当前的均值时，空值是否参与运算？然后假设现在把这些空值都用均值来填补。那么当样本数据更多，继续清洗数据时，之前被置为均值的那些空值，是否参与进下一次清洗时均值的计算？
>
> 若参与运算，算出来的均值肯定是不对的
>
> 若不参与运算，算出的其实是不为空值的那部分样本均值。那如何由样本均值推算总体均值？或者说这时候的样本均值是否能看成总体均值的无偏估计？



对于缺失值的，我们可以先计算占比多少，然后再根据不同的占比去做不同的处理

### 1、<=10%

​		缺失值占比 < 10%时，可以选择直接**抛弃**对应数据。

​		但需要注意的是，一条数据在某个特征上虽然为空值，但是其他特征没有缺失，这时候需要综合考虑其他特征来决定是否删除。并且删除数据后，对于整体数据集的质量在大部分情况下都会产生负面影响的。

​		所以这时候**更多的是选择不处理**。



### 2、<=X%

​		当缺失值占比 <= X%且>10%时，其实这时候数据的整体质量已经很差了，因为缺失的太多。

#### 一、算法填充

​		将空值的数据抽出来作为测试集A，剩下没有空值的数据作为训练集S。

​		将空值的那个特征作为标签，用S去训练一个分类器，再用这个分类器对A中的空值进行处理。

#### 二、默认值填充

​		若缺失列是数值型，我们通常以其他正常的数据计算均值，并以**均值**进行填充。

​		若确实列是离散型，则通常用**众**数，或者**中位数**进行填充



#### 3、 >X

​	当缺失率大过一个值时，我们可以直接将该特征抛弃掉。

​	**这个值我想通常是由经验得到。**





## 异常值

异常值多是指离群点，指数据集中明显不合理的点。比如年龄值是1000。

异常值的处理有两步：1、定位异常值，2、处理

### 定位异常值

#### 一、正态分布原理

​		如果当前特征满足正态分布，那么由其原理我们可知，绝大部分（占比99.73%）的数据会处在均值±3σ的范围内，那么不再这个范围内的数我们可以将其定为异常值

##### 		判断数据是否满足正态分布：

###### 			1、Q-Q图

###### 			2、直方图

###### 			3、偏态，峰态系数

​					若两个系数都小于1，则我们可以判断数据集近似服从正态分布

###### 			4、KS检验

​					基于样本的**累积分布函**数来判断。先计算出标准正态分布的累积分布函数，然后再计算样本集的累积分布函数，找出它们之间差值最大的点D，然后基于样本集的样本数和显著性水平找到差值边界值（类似t检验的边界值）。判断边界值和D的关系，**如果D小于边界值则可以认为样本的分布符合目标分布**。

​					python中有现成的包来进行KS检验：

```python
from scipy.stats import kstest 
kstest(x,cdf = "norm")
# x是待检验的数据集，cdf是目标分布，‘norm’表示正态分布
# cdf的取值有‘norm’,’expon’,’logistic’,’gumbel’,’gumbel_l’, gumbel_r’
```

###### 			5、AD检验

```python
from scipy.stats import anderson
anderson(x, dist='norm')
```

###### 			6、W检验

```python
from scipy.stats import shapiro
shapiro(x)  # 专门做正态分布校验的，所以只要传数据集。并且它不适合处理样本数＞5000的校验
```





#### 二、箱型图

​		将大于或小于箱型图上界或下界的点视为异常值

上四分位我们设为 **U**，表示的是所有样本中只有1/4的数值大于U 

同理，下四分位我们设为 **L**，表示的是所有样本中只有1/4的数值小于L

我们设上四分位与下四分位的差值为**IQR**，即：IQR=U-L

那么，**上界为 U+1.5IQR ，下界为： L - 1.5IQR**

#### 三、散点图

​		将数据展现在散点图上，能清晰的看到那些离群点

#### 四、经验、常识

​		如最开始提到的，如果一个人的年龄值为1000，那这肯定是异常的，人的年龄范围通常是在0~100，大于100的是极少数情况了

​		根据以往的经验与常识，来确定一个**范围**，不在这个范围内的都属于异常值



#### 五、[异常值处理](https://zhuanlan.zhihu.com/p/41528651)



### 处理

#### 一、不处理

#### 二、按缺失值处理

#### 三、删除

#### 四、用前后两个值的均值进行填充









## 标准化

​		当数据维度很多，且其中存在差值很大的数据时，该类数据会对结果产生决定性的影响。所以为了避免这种情况发生，我们通常需要对数据进行**归一化**处理。

​		比如使用KNN计算距离时，若其中一个特征的数值是其他特征的很多倍，那这个特征将其决定作用，很明显这不是我们想要的结果。将数据归一化后能够很好的提升许多算法的精度，同时对线性模型来说能提高收敛速度。



**常用的归一化方式有：**

### 1、min-max标准化

​		也称为离差标准化，是对原始数据的线性变换，**使结果值映射到[0 - 1]之间**。
$$
newValue = \frac{oldValue - min}{max-min}
$$


### 2、Z-score标准化

​		这种方法通过原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化，其实也就是计算标准分数。经过处理的数据符合标准正态分布，即均值为0，标准差为1，处理完之后的数据范围在**[-1, 1]**之间。转化函数为：
$$
newValue = \frac{oldValue-μ}{σ}
$$
​		其中μ是均值，σ是标准差





### 3、[数据转换](https://www.cnblogs.com/charlotte77/p/5622325.html)







## 特征处理，降维



### 1、PCA主成分分析





### 2、LDA线性判别分析





### 3、ICA独立成分分析



### 4、……
